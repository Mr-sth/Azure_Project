# -*- coding: utf-8 -*-
"""MyCOE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1my_nTTpQu5WybnF3jJF2_Yzgqws1dqu5

## MOUNTING GOOGLE DRIVE
"""

from google.colab import drive
drive.mount('/content/drive')

"""## IMPORTING"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

"""## FEATURE ENGINEERING FOR BENEFICIARY DATASET"""

#Load Beneficiaries (2008-2010)

beneficiaries = []

for year in [2008, 2009, 2010]:
    filename = f"/content/drive/MyDrive/GOOGLE COLAB/Dataset/New_Dataset_1/DE1_0_{year}_Beneficiary_Summary_File_Sample_20.csv"
    df = pd.read_csv(filename,dtype=str)
    print(filename)
    df['Year']=year
    beneficiaries.append(df)
beneficiaries = pd.concat(beneficiaries, ignore_index=True)
beneficiaries.shape

#Calculating age
beneficiaries['Bene_Age'] = pd.to_numeric(beneficiaries['Year']) - pd.to_numeric(beneficiaries['BENE_BIRTH_DT'].str[:4],errors='coerce')

# Sex 1=Male and 2=Female
beneficiaries['Sex'] = beneficiaries['BENE_SEX_IDENT_CD'].map({'1': 'M', '2': 'F'})

# ESRD indicator (binary flag)
beneficiaries['ESRD_FLAG']=beneficiaries['BENE_ESRD_IND'].map({'Y':1, '0':0})

# Counting the chronics counts
chronic_cols = [
    'SP_ALZHDMTA', 'SP_CHF', 'SP_CHRNKIDN', 'SP_CNCR', 'SP_COPD',
    'SP_DEPRESSN', 'SP_DIABETES', 'SP_ISCHMCHT', 'SP_OSTEOPRS',
    'SP_RA_OA', 'SP_STRKETIA'
]

# Count how many chronic conditions = 1
beneficiaries['CHRONIC_CNT'] = beneficiaries[chronic_cols].apply(lambda x: (x == '1').sum(), axis=1)

# Total Coverage months
beneficiaries['TOTAL_COVERAGE_MONS'] = (
    beneficiaries['BENE_HI_CVRAGE_TOT_MONS'].astype(float) +
    beneficiaries['BENE_SMI_CVRAGE_TOT_MONS'].astype(float) +
    beneficiaries['BENE_HMO_CVRAGE_TOT_MONS'].astype(float) +
    beneficiaries['PLAN_CVRG_MOS_NUM'].astype(float)
)

"""#FEATURE ENGINEERING FOR CARRIER DATASET"""

#Load Carrier Files

df_carrier = pd.read_csv("/content/drive/MyDrive/GOOGLE COLAB/Dataset/New_Dataset_1/DE1_0_2008_to_2010_Carrier_Claims_Sample_20A_1 - Copy-csv.csv", dtype=str)
df_carrier = df_carrier.sample(n=343827, random_state=42)
df_carrier.shape

# Duration Calculation
df_carrier['SERVICE_DURATION']= pd.to_numeric(df_carrier['CLM_THRU_DT'])-pd.to_numeric(df_carrier['CLM_FROM_DT']) #Output will be in the format of days

#Converting dates into numeric format
df_carrier["CLM_FROM_DT"] = pd.to_datetime(df_carrier["CLM_FROM_DT"], errors="coerce")
df_carrier["CLM_THRU_DT"] = pd.to_datetime(df_carrier["CLM_THRU_DT"], errors="coerce")

# Extracting year, month and day from CLM
df_carrier['FROM_YEAR']=df_carrier['CLM_FROM_DT'].dt.year
df_carrier['FROM_MONTH']=df_carrier['CLM_FROM_DT'].dt.month
df_carrier['FROM_DAY']=df_carrier['CLM_FROM_DT'].dt.day

df_carrier['THRU_YEAR']=df_carrier['CLM_THRU_DT'].dt.year
df_carrier['THRU_MONTH']=df_carrier['CLM_THRU_DT'].dt.month
df_carrier['THRU_DAY']=df_carrier['CLM_THRU_DT'].dt.day

#Diagnosis Code count
diagnosis_cols =[f'ICD9_DGNS_CD_{i}' for i in range(1,9)] # Corrected column names and range
df_carrier['diagnosis_count'] = df_carrier[diagnosis_cols].notna().sum(axis=1) # Use notna() and sum to count non-nulls

#Procedure counts
hcpcs_cols = [f'HCPCS_CD_{i}' for i in range(1,14)]
df_carrier['Num_Procedures']=df_carrier[hcpcs_cols].notna().sum(axis=1)

df_carrier[['HCPCS_CD_1','HCPCS_CD_2']].head()

# Amounts List
allowedamt_cols = [f'LINE_ALOWD_CHRG_AMT_{i}' for i in range(1,14)]
paidamt_cols = [f'LINE_NCH_PMT_AMT_{i}' for i in range(1,14)]
deductible_cols = [f'LINE_BENE_PTB_DDCTBL_AMT_{i}' for i in range(1, 14)]
primary_cols = [f'LINE_BENE_PRMRY_PYR_PD_AMT_{i}' for i in range(1, 14)]
coins_cols = [f'LINE_COINSRNC_AMT_{i}' for i in range(1, 14)]
df_carrier.head()

# Summing all amount wise
def safe_sum(row,cols):
  return pd.to_numeric(row[cols], errors='coerce').fillna(0).sum()

df_carrier['Total_Allowed'] = df_carrier.apply(lambda r: safe_sum(r,allowedamt_cols),axis=1)
df_carrier['Total_Paid'] = df_carrier.apply(lambda r: safe_sum(r,paidamt_cols),axis=1)
df_carrier['Total_Deductible'] = df_carrier.apply(lambda r: safe_sum(r, deductible_cols), axis=1)
df_carrier['Total_PrimaryPayer'] = df_carrier.apply(lambda r: safe_sum(r, primary_cols), axis=1)
df_carrier['Total_Coinsurance'] = df_carrier.apply(lambda r: safe_sum(r, coins_cols), axis=1)

# Ratios
import numpy as np
df_carrier['Payment_Ratio'] = df_carrier['Total_Paid'] / df_carrier['Total_Allowed'].replace(0, np.nan)
df_carrier['Patient_Responsibility'] = df_carrier['Total_Deductible'] + df_carrier['Total_Coinsurance']

# Selecting First physician only
df_carrier['Provider_ID'] = df_carrier['PRF_PHYSN_NPI_1'].astype(float)

# Suppose denial is when Total_Paid == 0 & Total_Allowed > 0
df_carrier['Denied'] = ((df_carrier['Total_Paid'] == 0) & (df_carrier['Total_Allowed'] > 0)).astype(int)

# Historical denial rate per provider
provider_denial = df_carrier.groupby('Provider_ID')['Denied'].mean().reset_index(name='Provider_Denial_Rate')
df_carrier = df_carrier.merge(provider_denial, on='Provider_ID', how='left')

"""## MERGED CLAIMS CARRIER AND BENEFICIARY DATASET"""

merged_df = pd.merge(df_carrier, beneficiaries, on="DESYNPUF_ID", how="left")
# Ensure DESYNPUF_ID is string type early
merged_df['DESYNPUF_ID'] = merged_df['DESYNPUF_ID'].astype(str)
merged_df.shape

merged_df['CLM_FROM_DT_days'] = merged_df['CLM_FROM_DT'].astype('int64') // (10**9 * 60 * 60 * 24)
merged_df['CLM_THRU_DT_days'] = merged_df['CLM_THRU_DT'].astype('int64') // (10**9 * 60 * 60 * 24)

carrier_required_cols = ['DESYNPUF_ID',
 'CLM_ID',
 'diagnosis_count',
 'Provider_ID',
 'Num_Procedures',
 'Total_Allowed',
 'Total_Paid',
 'Total_Deductible',
 'Total_PrimaryPayer',
 'Total_Coinsurance',
 #'diagnosis_count', # Removed duplicate
 'SERVICE_DURATION',
 'FROM_YEAR',
 'FROM_MONTH',
 'FROM_DAY',
 'THRU_YEAR',
 'THRU_MONTH',
 'THRU_DAY',
 'Payment_Ratio',
 'Patient_Responsibility',
 'Denied',
 'Provider_Denial_Rate',
 'BENE_HI_CVRAGE_TOT_MONS',
 'BENE_SMI_CVRAGE_TOT_MONS',
 'BENE_HMO_CVRAGE_TOT_MONS',
 'PLAN_CVRG_MOS_NUM',
 'Bene_Age',
 'Sex',
 'CHRONIC_CNT',
 'TOTAL_COVERAGE_MONS','ESRD_FLAG']

final_df = merged_df[carrier_required_cols]
final_df.info()

nan_cols = [
    'Provider_ID', 'Payment_Ratio', 'Provider_Denial_Rate',
    'Bene_Age', 'CHRONIC_CNT', 'TOTAL_COVERAGE_MONS', 'ESRD_FLAG'
]

final_df = final_df.dropna(subset=nan_cols)

# -----------------------------
# 1. Drop rows where ClaimStatus is missing (target variable)
# -----------------------------
final_df = final_df.dropna(subset=["Denied"])

# -----------------------------
# 2. Encode categorical variables
# -----------------------------
cat_cols = final_df.select_dtypes(include=["object"]).columns
encoders = {}
for col in cat_cols:
    le = LabelEncoder()
    final_df[col] = le.fit_transform(final_df[col].astype(str))
    encoders[col] = le

# -----------------------------
# 3. Define features & target
# -----------------------------
X = final_df.drop("Denied", axis=1)
y = final_df["Denied"]

# -----------------------------
# 4. Train-test split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# -----------------------------
# 5. Scale numeric features
# -----------------------------
X_train_num = X_train.select_dtypes(include=['int64', 'float64'])
X_test_num  = X_test.select_dtypes(include=['int64', 'float64'])

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_num)
X_test_scaled = scaler.transform(X_test_num)


# -----------------------------
# 6. Train model
# -----------------------------
model = LogisticRegression(max_iter=500, solver="lbfgs")
model.fit(X_train_scaled, y_train)


# -----------------------------
# 7. Evaluate
# -----------------------------
y_pred = model.predict(X_test_scaled)
print("✅ Accuracy:", accuracy_score(y_test, y_pred))

# -----------------------------
# 8. Predict probabilities
# -----------------------------
y_proba = model.predict_proba(X_test_scaled)[:, 1]


# -----------------------------
# 9. Build final output table
# -----------------------------
# Take original (non-encoded) values for readability
X_test_original = final_df.loc[X_test.index].copy()

results = pd.DataFrame({
    "DESYNPUF_ID": X_test_original["DESYNPUF_ID"],
    "BENE_AGE": X_test_original["Bene_Age"],
    "Provider": X_test_original["Provider_ID"],
    "Number Of Diagnosis": X_test_original["diagnosis_count"],
    "Number Of Procedures": X_test_original["Num_Procedures"],
    "Total Allowed Amount": X_test_original['Total_Allowed'],
    "Total Paid Amount": X_test_original['Total_Paid'],
    "Payment Ratio": X_test_original['Payment_Ratio'],
    "Chronic Count": X_test_original['CHRONIC_CNT'],
    "Provider Denial Rate": X_test_original['Provider_Denial_Rate'],
    "Predicted Risk": y_proba.round(2),
    "Status": ["⚠️ High Risk" if p >= 0.5 else "✅ Low Risk" for p in y_proba]
})

print("\n--- Sample Predictions ---\n")
results.head(20)